* re-hardwire medium pipeline size in the lib instead of having endpoint fields
* fix skbfrags checks for 8kb mediums

* constify the event queue as MX now does and mmap recvq+eventq's as RO

* only poll what's really need to be polled in the progression loop
  + only poll the exp event queue if some events are expected
  + only if jiffies changed
    - check enough progression
    - resend queue
    - partners to ack
  + only process delayed requests if something happened in other loops
  + what about check endpoint desc ?
  + make request_alloc_check() called in debug only

* optimize mtu 1500 by using more than 1024k medium or pull reply packets
  + break medium wire compat to add a medium frag length
  + ignore medium frag pipeline, and replace >> pipeline with using the frag length
  + update pull code to not assume the pull reply length is a pow2

* endpoint_close cannot be called from interrupt context
  - we can destroy region at first closing too?

* if killed while registering, needed to mark the region as failed?

* cleanup endpoint acquiring/locking/closing again
  + synchronize_rcu before freeing after unlinking
  + rcu_deference+acquire+check status during ioctl and network
  + stop allocating during the whole fd life
  + drop free/init status
  + drop the status_lock

* drop the iface endpoint array mutex and use the big ifaces mutex?
* drop the peer mutex as well since we have rcu everywhere?

* allow to probe across ctxids by adding a anyctxid unexpq

* cleanup the idr+BHlock for handles
  + sleep if not enough slot available

* fix/improve ioat support
  + sleep a bit before polling on synchronous copies
    - add an empirical ioat speed, tested at startup, tuned at runtime, configurable with a module param
    - use the empirical ioat speed to sleep using msleep during large 
      synchronous copy instead of polling
    - if copy completed on first poll, increase the speed by 12.5% or so
    - no need to reduce the speed ever

* support non-ethernet networks by providing ways to extract the mac address differently
  + use eth_hdr(), dev->header_ops->create() and ->parse() to clarify the accessing
    to eth headers of packets and make it less ethernet specific

* regcache malloc hook

* affinity
  + add OMX_BH_PREWARM_CACHE= to use non-temporal only if not on the process core
    - store the process core binding when opening the endpoint
      and assume the binding won't change if the env var was set

* huge page pinning support
  + add a pagesize in the region segment structure and use either pagesize of hugepagesize when possible
* support for architecture with pagesize != 4k
  + add an pgoffset in the sendq/recvq entry

* error handler when completing zombie send/recv only if ep totally open, with special message

* drop no-progression-during-one-second warning if no requests are pending?
  what's a pending request? anything but a posted recv?

* overlap registration with rndv and pull on the network
  + queue skb if touching after the registered_length
    - dequeue after registering enough
    - or just drop and let retransmission happen?
  + remove the user-space knowledge of window being registered for real
    - integrate into mmu notifier since there is no need anymore to notify userspace when region is invalidated

* resource allocation cleanups
  + add a counter of failure to alloc small buffer with ENOMEM and abort if too many?
  + add a counter of failure to pull with ENOMEM and abort if too many?

* if failing to deregister region
  *** glibc detected *** tests/omx_pingpong: malloc(): memory corruption: 0x000000000064edd0 ***

* 64bit internals by default
* 64bit API by default, unless OMX_32BITS_LENGTH is defined
  + how to force the #define OMX_32BITS_LENGTH once compiled and installed?
* --enable-32b/64b

* look at vringfd (http://lwn.net/Articles/276364/) to replace event ring

* endpoint parameters
  + unexp queue length

* don't free partner on disconnect since we can't change the session anymore?
  - or randomify the initial session?

* thread safety
  + filter wakeup on recv/send/connect/...
  + filter on specific request id too
  + progress thread only woken up if nobody else
  + split the progression timer out of the timeout timer and make it global
    and wakeup a single process
  + change the wakeup_jiffies mapped value into an ioctl parameter?
  + add a last_poll_jiffies so that the driver knows if the progress thread is needed after a timeout

* unlockify the ring management in the driver event reporting code
  + use rcu to avoid having to lock the waiter's queue
    - allocate the waiter struct and free it with a rcu_call
  + atomic_test_and_set instead of spinlock to get a event slot? what about recv slot?
    ok if lots of slices in parallel ?
  + add a driver-private field in event where the reporter will test_and_set BUSY
    before posting the event and finally posting the type and removing BUSY
  + per cpu rings? not scalable with number of cups
  + prefetch non-temporal from the event ring ro prevent cache line bounces?
  + or write event with non-temporal? hard in the kernel

* group similar fields in structure for cache effects, cache-align some fields?
  + add a counter per list and group them as well to reduce the progression loop overhead

* drop our region window offset ?
  + pull are aligned on the reader's pages because it's the slower side on native MX
    - we don't really care about alignment in open-mx, pages are pinned first, adding 2 pages to a skb isn't very expensive
    - we will still follow the remote side requirements when pulling from MX
    - MX doesn't care about our alignement when it pulls from us

* skb_clone and alloc_skb_fclone for pull and pull replies?

* wire compat
  + fix lib ack contents?

* optimize mediums
  + single ioctl, pipeline get_user_pages/dev_queue_xmit, put_pages in the last callback

* use large region id in the lib, only allocate uint8_t id for the wire when
  posting the rndv message or the pull request
* export rdma_get and rdma window management functions
* rdma_put
* write parameter in ioctl to register a region, check it when reading/writing from/to the region
  + different rdmawin id for sender/receiver
    - no need to check for deadlock if too many sender's rdmawin registered

* move recv lib in the kernel
  + early ioatcopy expected medium, move the other in the recv ring, drop when no space anymore
    - keep unexpected data in the ring for ever
    - when unexpected is posted, notify the kernel that we acquired the ring slots and let it
      finish receiving in the target buffer
  + limit unexpq size to avoid memory starvation
    - or allocate default unexp buffers upto 32kB
    - use a usual recv queue for unexp?
  + need to keep partner's recv_seqnum in the kernel
    - and synchronize with user-space which takes care of acking
    - share mapping of peer-index based array of recv seqnums between kernel and user-space?
  + use ioat to offload the whole recv medium copy and wait in the last frag
  + move the whole lib in the driver and only keep basic request status in userspace?

* add warning about mtu in nic/switches when truncated packet arrives, with printk_rate

=== obsolete ideas
* why shared memory from/to same send and recv buffer is very bad from 64k+ 128k ?
* change --iface into ---iface, and use --iface as a "close when no more endpoint"
* make NEED_SEQNUM a resource and use DELAYED for throttling as well ?
* what's in src_generation ? unset/unchecked in OMX
* wire-compat at runtime (8kB frame needs 17bits for medium length)
* req->recv.specific.medium.frags_received_mask and .accumulated_length are redundant
  + the mask is needed to detect duplicates, and the length prevents from precomputing the missing mask
* make shared comms runtime-configurable in the driver
